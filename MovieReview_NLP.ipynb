{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieReview_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darrenmcewan/IntroNLP/blob/main/MovieReview_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImO0uIZvzXdB"
      },
      "source": [
        "# Import Libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB7Gy4ZJVGe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2dd886-05a4-4b31-d48e-8e6ffb50ab80"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Cr_C_73gKg7"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iW8iQ9NY7vy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "outputId": "2026e5c2-5474-406e-e460-7b1e8a346d04"
      },
      "source": [
        "import nltk\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk import wordpunct_tokenize, word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "sw = stopwords.words('english')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, roc_curve"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-efff95750e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0whM1ROx1HDQ"
      },
      "source": [
        "from collections import defaultdict\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBhu3YvBgN1C"
      },
      "source": [
        "Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN_rrfMsgQIK"
      },
      "source": [
        "data = pd.read_csv(\"https://bd.darrenmcewan.com/ratings.tsv\", sep='\\t')\n",
        "data.review=data.review.astype(str)\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRoayQjgzjRg"
      },
      "source": [
        "# Data Cleaning and Split in Train/Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3A4vx_kw5_S"
      },
      "source": [
        "There were 55 rows that were empty. Dropping them leave us with 7945 rows of data to work with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt9k-czkh3Ex"
      },
      "source": [
        "#Drop blank rows\n",
        "data['review'].replace('', np.nan, inplace=True)\n",
        "data.dropna(subset=['review'],inplace=True)\n",
        "data.dropna(subset=['label'],inplace=True)\n",
        "\n",
        "#All words to lowercase\n",
        "data['review'] = [entry.lower() for entry in data['review']]\n",
        "\n",
        "#tokenization\n",
        "data['review']= [word_tokenize(entry) for entry in data['review']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVv50jMq02CJ"
      },
      "source": [
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "for index,entry in enumerate(data['review']):\n",
        "    # Declaring Empty List to store the words that follow the rules for this step\n",
        "    Final_words = []\n",
        "    # Initializing WordNetLemmatizer()\n",
        "    word_Lemmatized = WordNetLemmatizer()\n",
        "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
        "    for word, tag in pos_tag(entry):\n",
        "        # Below condition is to check for Stop words and consider only alphabets\n",
        "        if word not in stopwords.words('english') and word.isalpha():\n",
        "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "            Final_words.append(word_Final)\n",
        "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
        "    data.loc[index,'review_final'] = str(Final_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33cTHviExEvv"
      },
      "source": [
        "Split the data into train/test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyqV3nEulWI5"
      },
      "source": [
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(data['review_final'],data['label'],test_size=0.33,random_state=80)\n",
        "#y_train.dropna(inplace=True)\n",
        "#y_test.dropna(inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re4JXNYh4CIL"
      },
      "source": [
        "Encoder = LabelEncoder()\n",
        "y_train = Encoder.fit_transform(y_train)\n",
        "y_test = Encoder.fit_transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2_4sRvL7uCk"
      },
      "source": [
        "# Word Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixfSv2Lv7slY"
      },
      "source": [
        "Tfidf_vect = TfidfVectorizer(max_features=5000, stop_words='english', min_df=.01, max_df=.90)\n",
        "Tfidf_vect.fit(data['review_final'])\n",
        "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
        "print(Tfidf_vect.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggfY7Ya1ydL7"
      },
      "source": [
        "## TF-IDF Based on Count Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix0ZU-vtxM6n"
      },
      "source": [
        "This function will take in a dataset and use countVectorizer to make a df based on word and # of occurrences and also a df based on word and the term weights. I decided to return the 2nd df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxZEgqGlm6vZ"
      },
      "source": [
        "def tf_idf(dataset): \n",
        "\n",
        "  countVec = CountVectorizer(max_features= 5000, stop_words='english', min_df=.01, max_df=.90)\n",
        "\n",
        "  countVec.fit(dataset[\"review\"])\n",
        "  #useful debug, get an idea of the item list you generated\n",
        "  #list(countVec.vocabulary_.items())\n",
        "\n",
        "  countVec_count = countVec.transform(dataset[\"review\"])\n",
        "  occ = np.asarray(countVec_count.sum(axis=0)).ravel().tolist()\n",
        "\n",
        "  bowListFrame = pd.DataFrame({'term': countVec.get_feature_names(), 'occurrences': occ})\n",
        "  #print(bowListFrame)\n",
        "  #bowListFrame.sort_values(by='occurrences', ascending=False).head(60)\n",
        "\n",
        "  reviewTransformer = TfidfTransformer()\n",
        "\n",
        "  #initial fit representation using transformer object\n",
        "  reviewWeights = reviewTransformer.fit_transform(countVec_count)\n",
        "\n",
        "  #follow similar process to making new data frame with word occurrences, but with term weights\n",
        "  reviewWeightsFin = np.asarray(reviewWeights.mean(axis=0)).ravel().tolist()\n",
        "\n",
        "  #now that we've done Tfid, make a dataframe with weights and names\n",
        "  reviewWeightFrame = pd.DataFrame({'term': countVec.get_feature_names(), 'weight': reviewWeightsFin})\n",
        "  #print(reviewWeightFrame)\n",
        "  reviewWeightFrame=reviewWeightFrame.sort_values(by='weight', ascending=False).head(20)  \n",
        "  return reviewWeightFrame\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDhussPhpVZg"
      },
      "source": [
        "testWeightFrame = tf_idf(test)\n",
        "trainWeightFrame = tf_idf(train)\n",
        "\n",
        "print(trainWeightFrame)\n",
        "print(\"\\n\",testWeightFrame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "910ODLjFyICY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwkhqeKGyIzB"
      },
      "source": [
        "## TF-IDF Based on Tfidf Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTygtLUHyxg4"
      },
      "source": [
        "def plot_roc_curve(fpr, tpr):\n",
        "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
        "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7ljc2Xuy5BX"
      },
      "source": [
        "y_train = (train['label']=='neg').astype(int)\n",
        "y_test = (test['label']=='neg').astype(int)\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features= 5000, stop_words='english', min_df=.01, max_df=.90)\n",
        "tfidf.fit(train['review'])\n",
        "X_train = tfidf.transform(train['review'])\n",
        "X_test = tfidf.transform(test['review'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bcWEjD8zB8Z"
      },
      "source": [
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "yhat = nb.predict(X_test)\n",
        "print(accuracy_score(y_test, yhat))\n",
        "print(f1_score(y_test, yhat))\n",
        "print(precision_score(y_test, yhat))\n",
        "print(recall_score(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylAujIodzIBE"
      },
      "source": [
        "y_prob = nb.predict_proba(X_test)[:,1]\n",
        "fpr, tpr, thresh = roc_curve(y_test, y_prob)\n",
        "plot_roc_curve(fpr, tpr)\n",
        "print(roc_auc_score(y_test, y_prob))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iiHcXKwzJyq"
      },
      "source": [
        "feats = pd.DataFrame()\n",
        "feats['words'] = tfidf.get_feature_names()\n",
        "# Convert log probabilities to probabilities. \n",
        "feats['pos'] = np.e**(nb.feature_log_prob_[0, :])\n",
        "feats['neg'] = np.e**(nb.feature_log_prob_[1, :])\n",
        "feats.set_index('words', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWgPtVOLzLQq"
      },
      "source": [
        "feats.sort_values(by='pos',ascending=False).head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "463sj-0VzM4Z"
      },
      "source": [
        "feats.sort_values(by='neg',ascending=False).head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kZSCkjGEqWH"
      },
      "source": [
        "### Which words are most important for distinguishing between pos and neg reviews?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w5eObhBFCWx"
      },
      "source": [
        "Great, like, story, love, best, people are all words that had a high weight when it came to positive words. \n",
        "\n",
        "bad, just, time, plot, acting, and worst are words that scored high according to negative reviews. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdF1jLmD_6pf"
      },
      "source": [
        "#ML Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aiVHTGcDqsw"
      },
      "source": [
        "According to our score, SVM performed the best with an accuracy score of 87.19%, followed by NB with an accuracy score of 85%. MLP had the lowest score with 84%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCIJd2hkAC8o"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_p09qNd_-8g"
      },
      "source": [
        "# fit the training dataset on the NB classifier\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(Train_X_Tfidf,y_train)\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, y_test)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YduHn8BlA3CX"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vfC7q9iA7lc"
      },
      "source": [
        "# Classifier - Algorithm - SVM\n",
        "# fit the training dataset on the classifier\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf,y_train)\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "\n",
        "# Use accuracy_score function to get the accuracy\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oUDkwR_B4uD"
      },
      "source": [
        "## Multilayer Perceptron\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8hRXztqB7-n"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "MLP = MLPClassifier(solver='lbfgs', alpha=1e-3, random_state=3)\n",
        "MLP.fit(Train_X_Tfidf,y_train)\n",
        "\n",
        "\n",
        "predictions_MLP = MLP.predict(Test_X_Tfidf)\n",
        "print(\"MLP Accuracy Score -> \",accuracy_score(predictions_MLP, y_test)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL2gNMpSFlyq"
      },
      "source": [
        "# Vader Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnPzMhuUFp9_"
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIqRtxjuGQlC"
      },
      "source": [
        "def sentiment_scores(sentence):\n",
        "  sid_obj = SentimentIntensityAnalyzer()\n",
        "  sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "\n",
        "  # decide sentiment as positive, negative and neutral \n",
        "  if sentiment_dict['compound'] >= 0.05 : \n",
        "        return (\"pos\") \n",
        "  \n",
        "  elif sentiment_dict['compound'] <= - 0.05 : \n",
        "        return (\"neg\") \n",
        "  \n",
        "  else : \n",
        "        return (\"Neutral\") \n",
        "  #print(\"{:-<40} {}\".format(sentence, str(sentiment_dict)))"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-d7lG-oGoPn"
      },
      "source": [
        "vader=[]\n",
        "for i in range(len(data['review'])):\n",
        "  vader.append(sentiment_scores(data['review'][i]))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIs_Jl0DI0Yw"
      },
      "source": [
        "# Text Blob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZNHzejWI2-f"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def blobSentiment(text):\n",
        "  blob = TextBlob(text)\n",
        "  for sentence in blob.sentences:\n",
        "    print(sentence.sentiment.polarity)\n",
        "\n",
        "for i in range(10):\n",
        "  sentiment_scores(data['review_final'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihRagv2OKY_5"
      },
      "source": [
        "# Topic Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sEOcyDDKc4X"
      },
      "source": [
        "Run LDA topic modeling using gensim on the movie reviews. How many topics are there? What are\n",
        "the most common words in each topic?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e4S5LlSKb-_"
      },
      "source": [
        "# Gensim\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import lemmatize, simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apq7WvBRKs_f"
      },
      "source": [
        "ntopics = 8\n",
        "\n",
        "# Create Dictionary\n",
        "\n",
        "id2word = corpora.Dictionary(data['review'])\n",
        "\n",
        "# Create Corpus: Term Document Frequency\n",
        "\n",
        "corpus = [id2word.doc2bow(text) for text in data.review]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=ntopics, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=10,\n",
        "                                           passes=10,\n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=100,\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "ldatopics = lda_model.show_topics(formatted=False)\n",
        "print(lew(lda_model.print_topics()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}